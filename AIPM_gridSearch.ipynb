{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424d6eec-47ab-4821-8b4b-deefd6115a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417dbe39-2f5e-4692-8568-1a673b98ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04726277-4fdc-4c7e-b5ae-e9a499bdb542",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138bfd07-c0bf-4cb4-9692-a5ece36ea8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = r\"C:\\Users\\Alex\\Desktop\\AIPM\\dataset\"\n",
    "\n",
    "# Define image size\n",
    "image_size = (64, 64)\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=30,  # Random rotation between -30 and 30 degrees\n",
    "    horizontal_flip=True,  # Random horizontal flip\n",
    ")\n",
    "\n",
    "# This will store image data and labels\n",
    "metadata = []\n",
    "\n",
    "def preprocess_images_from_folder(folder_path, class_name):\n",
    "    \"\"\"\n",
    "    Process all images from a folder and store the image data and labels into the metadata list.\n",
    "    :param folder_path: The path to the folder containing images\n",
    "    :param class_name: The class name (folder name) of the images\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the image using OpenCV\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR (OpenCV) to RGB\n",
    "\n",
    "                # Resize the image to the target size\n",
    "                img_resized = cv2.resize(img, image_size)\n",
    "\n",
    "                # Normalize the image\n",
    "                img_resized_norm = img_resized / 255.0\n",
    "\n",
    "                # Add the image data (as a 3D array) and label to metadata\n",
    "                metadata.append({\n",
    "                    'image_data': img_resized_norm,\n",
    "                    'class': class_name\n",
    "                })\n",
    "\n",
    "                # Apply augmentation if necessary (not saving augmented images for CSV)\n",
    "                img_resized_norm = np.expand_dims(img_resized_norm, axis=0)  # Add batch dimension\n",
    "                augmented_gen = datagen.flow(img_resized_norm, batch_size=1)\n",
    "\n",
    "                for _ in range(1):  # Generate augmented images\n",
    "                    augmented_img = next(augmented_gen)[0]\n",
    "\n",
    "                    # Add augmented image data (as a 3D array) and label to metadata\n",
    "                    metadata.append({\n",
    "                        'image_data': augmented_img,\n",
    "                        'class': class_name\n",
    "                    })\n",
    "\n",
    "def process_dataset(dataset_path):\n",
    "    \"\"\"\n",
    "    Loop through the 'train' and 'test' directories and preprocess all images\n",
    "    :param dataset_path: The root path of the dataset\n",
    "    \"\"\"\n",
    "    # Process the 'train' folder\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        class_path = os.path.join(train_dir, class_name)\n",
    "        preprocess_images_from_folder(class_path, class_name)\n",
    "\n",
    "    # Process the 'test' folder\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "    for class_name in os.listdir(test_dir):\n",
    "        class_path = os.path.join(test_dir, class_name)\n",
    "        preprocess_images_from_folder(class_path, class_name)\n",
    "\n",
    "# Collect metadata and save to CSV\n",
    "def save_metadata_to_csv(metadata):\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv('image_metadata.csv', index=False)\n",
    "    print(\"Metadata saved to 'image_metadata.csv'\")\n",
    "\n",
    "# Save image data as a numpy file (.npz) containing arrays\n",
    "def save_metadata_to_npz(metadata, filename='image_data.npz'):\n",
    "    # Convert metadata to numpy arrays\n",
    "    images = np.array([entry['image_data'] for entry in metadata])\n",
    "    labels = np.array([entry['class'] for entry in metadata])\n",
    "\n",
    "    # Save both images and labels in a compressed .npz file\n",
    "    np.savez_compressed(filename, images=images, labels=labels)\n",
    "    print(f\"Metadata saved to {filename}\")\n",
    "\n",
    "\n",
    "# Run the preprocessing\n",
    "#process_dataset(dataset_path)\n",
    "\n",
    "# Usage\n",
    "#save_metadata_to_npz(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e4259e-08dc-4332-b5b8-b8c974e5d525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from the .npz file\n",
    "data = np.load('image_data.npz')\n",
    "X = data['images']  # This will have shape (num_samples, 64, 64, 3)\n",
    "y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5c1db6-e9cc-4b1a-8da9-aa4574eec62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convert text labels to numeric labels\n",
    "y = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y, num_classes=23)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36035a5c-4e89-4107-a3b4-eba81dadeb4a",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de49c992-de32-44cb-8903-847a99285233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12544)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1605760   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 23)                2967      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1628119 (6.21 MB)\n",
      "Trainable params: 1628119 (6.21 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(23, activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bc7ff13-1e77-4551-aa79-0f4bc84c33ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 42ms/step - accuracy: 0.0949 - loss: 3.0128 - val_accuracy: 0.1150 - val_loss: 2.9394\n",
      "Epoch 2/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.1204 - loss: 2.9257 - val_accuracy: 0.1305 - val_loss: 2.8834\n",
      "Epoch 3/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 45ms/step - accuracy: 0.1354 - loss: 2.8716 - val_accuracy: 0.1361 - val_loss: 2.8622\n",
      "Epoch 4/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.1471 - loss: 2.8236 - val_accuracy: 0.1443 - val_loss: 2.8507\n",
      "Epoch 5/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.1669 - loss: 2.7698 - val_accuracy: 0.1559 - val_loss: 2.8259\n",
      "Epoch 6/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.1762 - loss: 2.7126 - val_accuracy: 0.1540 - val_loss: 2.8200\n",
      "Epoch 7/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.1902 - loss: 2.6525 - val_accuracy: 0.1595 - val_loss: 2.8235\n",
      "Epoch 8/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.2069 - loss: 2.5932 - val_accuracy: 0.1607 - val_loss: 2.8480\n",
      "Epoch 9/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 42ms/step - accuracy: 0.2301 - loss: 2.5091 - val_accuracy: 0.1702 - val_loss: 2.8746\n",
      "Epoch 10/10\n",
      "\u001b[1m783/783\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.2470 - loss: 2.4473 - val_accuracy: 0.1676 - val_loss: 2.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Split data manually (instead of using validation_split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model with explicit validation data\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save('skin_disease_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ea6259e-2266-4388-b544-e34d1967754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.1739 - loss: 2.8504\n",
      "Test Accuracy: 16.99%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e8ee33-da27-4250-8c8e-841d02da72b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 31, 31, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 14, 14, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               589952    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 23)                2967      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 686167 (2.62 MB)\n",
      "Trainable params: 686167 (2.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional and pooling layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add fully connected layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout for regularization\n",
    "model.add(Dense(23, activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cf2faff-3858-4ed7-a259-f817a5e776fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 39ms/step - accuracy: 0.0842 - loss: 3.0218 - val_accuracy: 0.0985 - val_loss: 2.9340\n",
      "Epoch 2/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.1060 - loss: 2.9567 - val_accuracy: 0.1148 - val_loss: 2.9110\n",
      "Epoch 3/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 39ms/step - accuracy: 0.1235 - loss: 2.9143 - val_accuracy: 0.1308 - val_loss: 2.8849\n",
      "Epoch 4/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.1267 - loss: 2.8916 - val_accuracy: 0.1390 - val_loss: 2.8654\n",
      "Epoch 5/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 38ms/step - accuracy: 0.1458 - loss: 2.8517 - val_accuracy: 0.1450 - val_loss: 2.8341\n",
      "Epoch 6/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.1473 - loss: 2.8140 - val_accuracy: 0.1496 - val_loss: 2.8170\n",
      "Epoch 7/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.1514 - loss: 2.7915 - val_accuracy: 0.1576 - val_loss: 2.7918\n",
      "Epoch 8/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 41ms/step - accuracy: 0.1740 - loss: 2.7475 - val_accuracy: 0.1845 - val_loss: 2.7275\n",
      "Epoch 9/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 40ms/step - accuracy: 0.1966 - loss: 2.6843 - val_accuracy: 0.1883 - val_loss: 2.7233\n",
      "Epoch 10/10\n",
      "\u001b[1m626/626\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 39ms/step - accuracy: 0.2098 - loss: 2.6273 - val_accuracy: 0.2003 - val_loss: 2.6806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Split data manually (instead of using validation_split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model with explicit validation data\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "model.save('skin_disease_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f91d8d89-3007-486c-9428-b01f08a9fdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.2078 - loss: 2.6638\n",
      "Test Accuracy: 20.30%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafadc29-df4a-4be6-b83f-9d2c79e4bb4a",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a6a94e-42c2-401c-922d-daa8cff8a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def create_model(neuron=32, optimizator='adam', dropout_rate=0.0):\n",
    "    model = Sequential([\n",
    "        Dense(neuron, input_dim=X_train.shape[1], activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(np.unique(y)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizator, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4e38e2e-570f-411e-bf3a-5f955ee0f112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing DecisionTree...\n",
      "Best hyperparameters for DecisionTree: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\n",
      "Optimizing RandomForest...\n",
      "Best hyperparameters for RandomForest: {'max_depth': 10, 'n_estimators': 50}\n",
      "\n",
      "Optimizing GradientBoosting...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 24 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 672, in fit\n    y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1485, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (666, 23) instead.\n\n--------------------------------------------------------------------------------\n16 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 672, in fit\n    y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1485, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (667, 23) instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 50\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     44\u001b[0m     model,\n\u001b[0;32m     45\u001b[0m     param_grids[name],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[1;32m---> 50\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_2d\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m best_models[name] \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1001\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    995\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    997\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    999\u001b[0m     )\n\u001b[1;32m-> 1001\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    511\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    512\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m     )\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 24 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n8 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 672, in fit\n    y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1485, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (666, 23) instead.\n\n--------------------------------------------------------------------------------\n16 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 672, in fit\n    y = column_or_1d(y, warn=True)  # TODO: Is this still required?\n  File \"C:\\Users\\Alex\\anaconda3\\envs\\tf_env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1485, in column_or_1d\n    raise ValueError(\nValueError: y should be a 1d array, got an array of shape (667, 23) instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D\n",
    "\n",
    "# Defining the models to be used\n",
    "random_state = 42\n",
    "\n",
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=random_state),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=random_state)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"DecisionTree\": {\n",
    "        \"max_depth\": [3, 5, 10],\n",
    "        \"min_samples_split\": [2, 5],\n",
    "        \"min_samples_leaf\": [1, 2]\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [50, 100],\n",
    "        \"max_depth\": [5, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "# Grid Search for each model\n",
    "for name, model in models.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        model,\n",
    "        param_grids[name],\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        cv=3,  # Reduce the number of folds for faster execution\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X_train_2d[:1000], y_train[:1000])\n",
    "    \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {name}: {grid_search.best_params_}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959a0504-080b-467b-bef0-75a025c6dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "X_train_2d = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_2d = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Best hyperparameters for DecisionTree and RandomForest from GridSearchCV\n",
    "best_dt = DecisionTreeRegressor(max_depth=10, min_samples_leaf=2, min_samples_split=5)\n",
    "best_rf = RandomForestRegressor(max_depth=10, n_estimators=50)\n",
    "\n",
    "# Train DecisionTreeRegressor with the best parameters\n",
    "best_dt.fit(X_train_2d, y_train)\n",
    "y_pred_dt = best_dt.predict(X_test_2d)\n",
    "\n",
    "# Train RandomForestRegressor with the best parameters\n",
    "best_rf.fit(X_train_2d, y_train)\n",
    "y_pred_rf = best_rf.predict(X_test_2d)\n",
    "\n",
    "# Evaluate models\n",
    "print(\"DecisionTreeRegressor Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_dt)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_dt)}\")\n",
    "print(f\"R^2 Score: {r2_score(y_test, y_pred_dt)}\\n\")\n",
    "\n",
    "print(\"RandomForestRegressor Evaluation:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred_rf)}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred_rf)}\")\n",
    "print(f\"R^2 Score: {r2_score(y_test, y_pred_rf)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b2f8b-b704-4a38-b7e7-b0ee3731d768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
